# -*- coding: utf-8 -*-
"""mbti_character_crawling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13dliMbDPn2BN_f-huI7LuQiOb4LEM8QJ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

def crawl_character_info(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }

    session = requests.Session()
    session.headers.update(headers)

    try:
        response = session.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Error fetching the webpage: {e}")
        return None

    soup = BeautifulSoup(response.content, 'html.parser')

    character_sections = soup.find_all('h2')
    data = []
    unwanted_headers = ['The Blog', 'INFJ fictional characters']

    for i, char_section in enumerate(character_sections):
        char_name = char_section.get_text().strip()

        if any(unwanted in char_name for unwanted in unwanted_headers):
            continue

        char_desc_div = char_section.find_next_sibling('div', class_="image-title-desc__desc")
        char_text = char_desc_div.get_text().strip() if char_desc_div else ""

        data.append({
            'character_name': char_name,
            'mbti': 'INFJ',
            'text': char_text
        })

    dataset = pd.DataFrame(data)
    return dataset

url = "https://www.sosyncd.com/infj-fictional-characters/"
dataset = crawl_character_info(url)

if dataset is not None:
    print(dataset)

if dataset is not None:
    # Define the JSON file path
    json_file_path = './infj_character_data.json'
    # Save to JSON file
    dataset.to_json(json_file_path, orient='records', lines=True, force_ascii=False)
    json_file_path
else:
    print("Failed to fetch or save data.")

